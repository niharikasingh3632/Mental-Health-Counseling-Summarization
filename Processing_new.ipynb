{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsyDtFH/T2K2m4qpIBf4Q/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niharikasingh3632/Mental-Health-Counseling-Summarization/blob/main/Processing_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZh1l5ohkbNE",
        "outputId": "3b0ecc9d-d581-4c38-e553-61fea73f83ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy inflect\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "ufV6csmyxcj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load model + tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "\n",
        "# Label mapping\n",
        "label_map = {\n",
        "    0: \"negative\",\n",
        "    1: \"neutral\",\n",
        "    2: \"positive\"\n",
        "}\n",
        "\n",
        "# Grammar transformation using regex\n",
        "def grammar_replace(text: str, role: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    replacements = [\n",
        "        (r\"\\b[Ii]['’`]?m not\\b\", f\"{role} is not\"),\n",
        "        (r\"\\b[Ii]['’`]?m\\b\", f\"{role} is\"),\n",
        "        (r\"\\b[Ii]\\s+am\\b\", f\"{role} is\"),\n",
        "        (r\"\\b[Ii]\\s+have\\b\", f\"{role} has\"),\n",
        "        (r\"\\b[Ii]\\s+haven't\\b\", f\"{role} has not\"),\n",
        "        (r\"\\b[Ii]\\s+was\\b\", f\"{role} was\"),\n",
        "        (r\"\\b[Ii]\\s+do\\b\", f\"{role} does\"),\n",
        "        (r\"\\b[Ii]\\s+don't\\b\", f\"{role} doesn't\"),\n",
        "        (r\"\\b[Ii]\\s+can\\b\", f\"{role} can\"),\n",
        "        (r\"\\b[Ii]\\s+will\\b\", f\"{role} will\"),\n",
        "        (r\"\\b[Ii]['’]?ve\\b\", f\"{role} has\"),\n",
        "        (r\"\\b[Ii]['’]?d\\b\", f\"{role} would\"),\n",
        "        (r\"\\b[Ii]['’]?ll\\b\", f\"{role} will\"),\n",
        "        (r\"\\b[Mm]y\\b\", f\"{role}'s\"),\n",
        "        (r\"\\b[Mm]ine\\b\", f\"{role}'s\"),\n",
        "        (r\"\\b[Mm]e\\b\", f\"{role}\"),\n",
        "        (r\"\\b[Mm]yself\\b\", f\"{role} himself\"),\n",
        "        (r\"\\b[Ii]\\b\", f\"{role}\"),\n",
        "    ]\n",
        "\n",
        "    for pattern, repl in replacements:\n",
        "        text = re.sub(pattern, repl, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Replaces \"you\" and \"your\" based on speaker role\n",
        "def replace_you_and_your(text: str, role: str) -> str:\n",
        "    if role == \"Patient\":\n",
        "        other = \"therapist\"\n",
        "    else:\n",
        "        other = \"patient\"\n",
        "\n",
        "    text = re.sub(r\"\\byou're\\b\", f\"{other} is\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\byou'd\\b\", f\"{other} had\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\byou've\\b\", f\"{other} has\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\byour\\b\", f\"{other}'s\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\byou\\b\", other, text, flags=re.IGNORECASE)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Sentiment analysis\n",
        "def get_sentiment(text: str) -> str:\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"neutral\"\n",
        "    try:\n",
        "        encoded_input = tokenizer(text, return_tensors='pt', truncation=True)\n",
        "        output = model(**encoded_input)\n",
        "        scores = torch.nn.functional.softmax(output.logits, dim=1)\n",
        "        predicted_label = torch.argmax(scores).item()\n",
        "        return label_map[predicted_label]\n",
        "    except Exception:\n",
        "        return \"neutral\"\n",
        "\n",
        "# Preprocess CSV files\n",
        "def preprocess_csv_files(input_path, output_path, log_file_path):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    processed_files = []\n",
        "\n",
        "    for filename in os.listdir(input_path):\n",
        "        if not filename.endswith(\".csv\"):\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(input_path, filename)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding=\"utf-8\", sep=None, engine='python')\n",
        "            df.columns = df.columns.str.strip()\n",
        "\n",
        "            if \"Utterance\" not in df.columns or \"Sub topic\" not in df.columns:\n",
        "                raise KeyError(f\"Missing required columns in: {filename}\")\n",
        "\n",
        "            metadata_df = df.iloc[-3:].copy() if len(df) >= 3 else pd.DataFrame()\n",
        "            dialogue_df = df.iloc[:-3].copy() if len(df) >= 3 else df.copy()\n",
        "\n",
        "            dialogue_df = dialogue_df[~dialogue_df[\"Sub topic\"].str.lower().eq(\"inactive\")]\n",
        "\n",
        "            if dialogue_df.empty:\n",
        "                print(f\"⚠️ No valid dialogue rows in {filename}\")\n",
        "                continue\n",
        "\n",
        "            if \"Type\" not in dialogue_df.columns:\n",
        "                raise KeyError(f\"Missing 'Type' column in: {filename}\")\n",
        "\n",
        "            def process_row(row):\n",
        "                utterance = str(row[\"Utterance\"])\n",
        "                role = \"Patient\" if row[\"Type\"] == \"P\" else \"Therapist\"\n",
        "\n",
        "                # Transform text\n",
        "                transformed = grammar_replace(utterance, role)\n",
        "                transformed = replace_you_and_your(transformed, role)\n",
        "                transformed = transformed.lower()\n",
        "\n",
        "                # Get sentiment of original utterance (optional: could also analyze transformed)\n",
        "                sentiment = get_sentiment(utterance)\n",
        "\n",
        "                return pd.Series([transformed, sentiment])\n",
        "\n",
        "            dialogue_df[[\"Utterance\", \"Sentiment\"]] = dialogue_df.apply(process_row, axis=1)\n",
        "\n",
        "            final_df = pd.concat([dialogue_df, metadata_df], ignore_index=True)\n",
        "            output_file = os.path.join(output_path, filename)\n",
        "            final_df.to_csv(output_file, index=False)\n",
        "            processed_files.append(os.path.splitext(filename)[0])\n",
        "            print(f\"Saved: {output_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    with open(log_file_path, \"w\") as f:\n",
        "        for name in processed_files:\n",
        "            f.write(name + \"\\n\")\n",
        "\n",
        "    print(f\"\\n Done! {len(processed_files)} files processed.\")\n",
        "    print(f\"Log saved to: {log_file_path}\")\n"
      ],
      "metadata": {
        "id": "3uX1pwHkkfcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Grammar transformation using regex\n",
        "def grammar_replace(text: str, role: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    replacements = [\n",
        "        (r\"\\b[Ii]['’`]?m not\\b\", f\"{role} is not\"),\n",
        "        (r\"\\b[Ii]['’`]?m\\b\", f\"{role} is\"),\n",
        "        (r\"\\b[Ii]\\s+am\\b\", f\"{role} is\"),\n",
        "        (r\"\\b[Ii]\\s+have\\b\", f\"{role} has\"),\n",
        "        (r\"\\b[Ii]\\s+haven't\\b\", f\"{role} has not\"),\n",
        "        (r\"\\b[Ii]\\s+was\\b\", f\"{role} was\"),\n",
        "        (r\"\\b[Ii]\\s+do\\b\", f\"{role} does\"),\n",
        "        (r\"\\b[Ii]\\s+don't\\b\", f\"{role} doesn't\"),\n",
        "        (r\"\\b[Ii]\\s+can\\b\", f\"{role} can\"),\n",
        "        (r\"\\b[Ii]\\s+will\\b\", f\"{role} will\"),\n",
        "        (r\"\\b[Ii]['’]?ve\\b\", f\"{role} has\"),\n",
        "        (r\"\\b[Ii]['’]?d\\b\", f\"{role} would\"),\n",
        "        (r\"\\b[Ii]['’]?ll\\b\", f\"{role} will\"),\n",
        "        (r\"\\b[Mm]y\\b\", f\"{role}'s\"),\n",
        "        (r\"\\b[Mm]ine\\b\", f\"{role}'s\"),\n",
        "        (r\"\\b[Mm]e\\b\", f\"{role}\"),\n",
        "        (r\"\\b[Mm]yself\\b\", f\"{role} himself\"),\n",
        "        (r\"\\b[Ii]\\b\", f\"{role}\"),\n",
        "    ]\n",
        "\n",
        "    for pattern, repl in replacements:\n",
        "        text = re.sub(pattern, repl, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Replaces \"you\" and \"your\" based on speaker role\n",
        "def replace_you_and_your(text: str, role: str) -> str:\n",
        "    if role == \"Patient\":\n",
        "        other = \"therapist\"\n",
        "    else:\n",
        "        other = \"patient\"\n",
        "\n",
        "    text = re.sub(r\"\\byou're\\b\", f\"{other} is\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\byou'd\\b\", f\"{other} had\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\byou've\\b\", f\"{other} has\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\byour\\b\", f\"{other}'s\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\byou\\b\", other, text, flags=re.IGNORECASE)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Preprocess CSV files\n",
        "def preprocess_csv_files(input_path, output_path, log_file_path):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    processed_files = []\n",
        "\n",
        "    for filename in os.listdir(input_path):\n",
        "        if not filename.endswith(\".csv\"):\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(input_path, filename)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding=\"utf-8\", sep=None, engine='python')\n",
        "            df.columns = df.columns.str.strip()\n",
        "\n",
        "            if \"Utterance\" not in df.columns or \"Sub topic\" not in df.columns:\n",
        "                raise KeyError(f\"Missing required columns in: {filename}\")\n",
        "\n",
        "            metadata_df = df.iloc[-3:].copy() if len(df) >= 3 else pd.DataFrame()\n",
        "            dialogue_df = df.iloc[:-3].copy() if len(df) >= 3 else df.copy()\n",
        "\n",
        "            dialogue_df = dialogue_df[~dialogue_df[\"Sub topic\"].str.lower().eq(\"inactive\")]\n",
        "\n",
        "            if dialogue_df.empty:\n",
        "                print(f\"⚠️ No valid dialogue rows in {filename}\")\n",
        "                continue\n",
        "\n",
        "            if \"Type\" not in dialogue_df.columns:\n",
        "                raise KeyError(f\"Missing 'Type' column in: {filename}\")\n",
        "\n",
        "            def process_row(row):\n",
        "                utterance = str(row[\"Utterance\"])\n",
        "                role = \"Patient\" if row[\"Type\"] == \"P\" else \"Therapist\"\n",
        "\n",
        "                transformed = grammar_replace(utterance, role)\n",
        "                transformed = replace_you_and_your(transformed, role)\n",
        "                transformed = transformed.lower()\n",
        "\n",
        "                return transformed\n",
        "\n",
        "            dialogue_df[\"Utterance\"] = dialogue_df.apply(lambda row: process_row(row), axis=1)\n",
        "\n",
        "            final_df = pd.concat([dialogue_df, metadata_df], ignore_index=True)\n",
        "            output_file = os.path.join(output_path, filename)\n",
        "            final_df.to_csv(output_file, index=False)\n",
        "            processed_files.append(os.path.splitext(filename)[0])\n",
        "            print(f\" Saved: {output_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    with open(log_file_path, \"w\") as f:\n",
        "        for name in processed_files:\n",
        "            f.write(name + \"\\n\")\n",
        "\n",
        "    print(f\"\\n Done! {len(processed_files)} files processed.\")\n",
        "    print(f\"Log saved to: {log_file_path}\")\n"
      ],
      "metadata": {
        "id": "SgbqIOIJSn27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_path = \"/content/drive/MyDrive/MEMO_KDD_2022/Train\"\n",
        "output_path = \"/content/drive/MyDrive/MEMO_KDD_2022/Processed_new/training\"\n",
        "train = \"/content/drive/MyDrive/MEMO_KDD_2022/Processed_new/train.txt\"\n",
        "\n",
        "preprocess_csv_files(input_path, output_path, train)\n"
      ],
      "metadata": {
        "id": "_teW8ktCR2ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/drive/MyDrive/MEMO_KDD_2022/Validation\"\n",
        "output_path = \"/content/drive/MyDrive/MEMO_KDD_2022/Processed_new/validating\"\n",
        "train = \"/content/drive/MyDrive/MEMO_KDD_2022/Processed_new/val.txt\"\n",
        "\n",
        "preprocess_csv_files(input_path, output_path, train)\n"
      ],
      "metadata": {
        "id": "x6hk86ZmSFom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/drive/MyDrive/MEMO_KDD_2022/Test\"\n",
        "output_path = \"/content/drive/MyDrive/MEMO_KDD_2022/Processed_new/testing\"\n",
        "train = \"/content/drive/MyDrive/MEMO_KDD_2022/Processed_new/test.txt\"\n",
        "\n",
        "preprocess_csv_files(input_path, output_path, train)\n"
      ],
      "metadata": {
        "id": "2CK-bPwcSN5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load model + tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "\n",
        "# Label mapping\n",
        "label_map = {\n",
        "    0: \"negative\",\n",
        "    1: \"neutral\",\n",
        "    2: \"positive\"\n",
        "}\n",
        "\n",
        "# Grammar transformation using regex\n",
        "def grammar_replace(text: str, role: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    replacements = [\n",
        "        (r\"\\b[Ii]['’`]?m not\\b\", f\"{role} is not\"),\n",
        "        (r\"\\b[Ii]['’`]?m\\b\", f\"{role} is\"),\n",
        "        (r\"\\b[Ii]\\s+am\\b\", f\"{role} is\"),\n",
        "        (r\"\\b[Ii]\\s+have\\b\", f\"{role} has\"),\n",
        "        (r\"\\b[Ii]\\s+haven't\\b\", f\"{role} has not\"),\n",
        "        (r\"\\b[Ii]\\s+was\\b\", f\"{role} was\"),\n",
        "        (r\"\\b[Ii]\\s+do\\b\", f\"{role} does\"),\n",
        "        (r\"\\b[Ii]\\s+don't\\b\", f\"{role} doesn't\"),\n",
        "        (r\"\\b[Ii]\\s+can\\b\", f\"{role} can\"),\n",
        "        (r\"\\b[Ii]\\s+will\\b\", f\"{role} will\"),\n",
        "        (r\"\\b[Ii]['’]?ve\\b\", f\"{role} has\"),\n",
        "        (r\"\\b[Ii]['’]?d\\b\", f\"{role} would\"),\n",
        "        (r\"\\b[Ii]['’]?ll\\b\", f\"{role} will\"),\n",
        "        (r\"\\b[Mm]y\\b\", f\"{role}'s\"),\n",
        "        (r\"\\b[Mm]ine\\b\", f\"{role}'s\"),\n",
        "        (r\"\\b[Mm]e\\b\", f\"{role}\"),\n",
        "        (r\"\\b[Mm]yself\\b\", f\"{role} himself\"),\n",
        "        (r\"\\b[Ii]\\b\", f\"{role}\"),\n",
        "    ]\n",
        "\n",
        "    for pattern, repl in replacements:\n",
        "        text = re.sub(pattern, repl, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Verb conjugation for simple subject-verb agreement\n",
        "def conjugate_verb(verb: str) -> str:\n",
        "    if verb.endswith('y') and len(verb) > 1 and verb[-2] not in 'aeiou':\n",
        "        return verb[:-1] + 'ies'\n",
        "    elif verb.endswith(('s', 'sh', 'ch', 'x', 'z', 'o')):\n",
        "        return verb + 'es'\n",
        "    else:\n",
        "        return verb + 's'\n",
        "\n",
        "# Sentiment analysis\n",
        "def get_sentiment(text: str) -> str:\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"neutral\"\n",
        "    try:\n",
        "        encoded_input = tokenizer(text, return_tensors='pt', truncation=True)\n",
        "        output = model(**encoded_input)\n",
        "        scores = torch.nn.functional.softmax(output.logits, dim=1)\n",
        "        predicted_label = torch.argmax(scores).item()\n",
        "        return label_map[predicted_label]\n",
        "    except Exception:\n",
        "        return \"neutral\"\n",
        "\n",
        "# Preprocess CSV files\n",
        "def preprocess_csv_files(input_path, output_path, log_file_path):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    processed_files = []\n",
        "\n",
        "    for filename in os.listdir(input_path):\n",
        "        if not filename.endswith(\".csv\"):\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(input_path, filename)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding=\"utf-8\", sep=None, engine='python')\n",
        "            df.columns = df.columns.str.strip()\n",
        "\n",
        "            if \"Utterance\" not in df.columns or \"Sub topic\" not in df.columns:\n",
        "                raise KeyError(f\"Missing required columns in: {filename}\")\n",
        "\n",
        "            # Split metadata\n",
        "            metadata_df = df.iloc[-3:].copy() if len(df) >= 3 else pd.DataFrame()\n",
        "            dialogue_df = df.iloc[:-3].copy() if len(df) >= 3 else df.copy()\n",
        "\n",
        "            # Filter inactive rows\n",
        "            dialogue_df = dialogue_df[~dialogue_df[\"Sub topic\"].str.lower().eq(\"inactive\")]\n",
        "\n",
        "            if dialogue_df.empty:\n",
        "                print(f\"⚠️ No valid dialogue rows in {filename}\")\n",
        "                continue\n",
        "\n",
        "            if \"Type\" not in dialogue_df.columns:\n",
        "                raise KeyError(f\"Missing 'Type' column in: {filename}\")\n",
        "\n",
        "            def process_row(row):\n",
        "                utterance = str(row[\"Utterance\"])\n",
        "                role = \"Patient\" if row[\"Type\"] == \"P\" else \"Therapist\"\n",
        "                transformed = grammar_replace(utterance, role)\n",
        "                sentiment = get_sentiment(utterance)\n",
        "                return pd.Series([transformed, sentiment])\n",
        "\n",
        "            dialogue_df[[\"Utterance\", \"Sentiment\"]] = dialogue_df.apply(process_row, axis=1)\n",
        "\n",
        "            final_df = pd.concat([dialogue_df, metadata_df], ignore_index=True)\n",
        "            output_file = os.path.join(output_path, filename)\n",
        "            final_df.to_csv(output_file, index=False)\n",
        "            processed_files.append(os.path.splitext(filename)[0])\n",
        "            print(f\"Saved: {output_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    # Save log\n",
        "    with open(log_file_path, \"w\") as f:\n",
        "        for name in processed_files:\n",
        "            f.write(name + \"\\n\")\n",
        "\n",
        "    print(f\"\\n Done! {len(processed_files)} files processed.\")\n",
        "    print(f\"Log saved to: {log_file_path}\")\n"
      ],
      "metadata": {
        "id": "M29vsR8lF0yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Grammar transformation using regex\n",
        "def grammar_replace(text: str, role: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    replacements = [\n",
        "        (r\"\\b[Ii]['’`]?m not\\b\", f\"{role} is not\"),\n",
        "        (r\"\\b[Ii]['’`]?m\\b\", f\"{role} is\"),\n",
        "        (r\"\\b[Ii]\\s+am\\b\", f\"{role} is\"),\n",
        "        (r\"\\b[Ii]\\s+have\\b\", f\"{role} has\"),\n",
        "        (r\"\\b[Ii]\\s+haven't\\b\", f\"{role} has not\"),\n",
        "        (r\"\\b[Ii]\\s+was\\b\", f\"{role} was\"),\n",
        "        (r\"\\b[Ii]\\s+do\\b\", f\"{role} does\"),\n",
        "        (r\"\\b[Ii]\\s+don't\\b\", f\"{role} doesn't\"),\n",
        "        (r\"\\b[Ii]\\s+can\\b\", f\"{role} can\"),\n",
        "        (r\"\\b[Ii]\\s+will\\b\", f\"{role} will\"),\n",
        "        (r\"\\b[Ii]['’]?ve\\b\", f\"{role} has\"),\n",
        "        (r\"\\b[Ii]['’]?d\\b\", f\"{role} would\"),\n",
        "        (r\"\\b[Ii]['’]?ll\\b\", f\"{role} will\"),\n",
        "        (r\"\\b[Mm]yself\\b\", f\"{role} himself\"),\n",
        "        (r\"\\b[Mm]y\\b\", f\"{role}'s\"),\n",
        "        (r\"\\b[Mm]ine\\b\", f\"{role}'s\"),\n",
        "        (r\"\\b[Mm]e\\b\", f\"{role}\"),\n",
        "        (r\"\\b[Ii]\\b\", f\"{role}\"),\n",
        "    ]\n",
        "\n",
        "    for pattern, repl in replacements:\n",
        "        text = re.sub(pattern, repl, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# You/Your replacement\n",
        "def replace_you_and_your(text: str, role: str) -> str:\n",
        "    if role == \"Patient\":\n",
        "        other = \"therapist\"\n",
        "    else:\n",
        "        other = \"patient\"\n",
        "\n",
        "    # Specific contractions first\n",
        "    text = re.sub(r\"\\byou're\\b\", f\"{other} is\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\byou'd\\b\", f\"{other} had\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\byou've\\b\", f\"{other} has\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Then more general replacements\n",
        "    text = re.sub(r\"\\byour\\b\", f\"{other}'s\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\byou\\b\", other, text, flags=re.IGNORECASE)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# Verb conjugation helper\n",
        "def conjugate_verb(verb: str) -> str:\n",
        "    if verb.endswith('y') and len(verb) > 1 and verb[-2] not in 'aeiou':\n",
        "        return verb[:-1] + 'ies'\n",
        "    elif verb.endswith(('s', 'sh', 'ch', 'x', 'z', 'o')):\n",
        "        return verb + 'es'\n",
        "    else:\n",
        "        return verb + 's'\n",
        "\n",
        "# Main preprocessing function\n",
        "def preprocess_csv_files(input_path, output_path, log_file_path):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    processed_files = []\n",
        "\n",
        "    for filename in os.listdir(input_path):\n",
        "        if not filename.endswith(\".csv\"):\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(input_path, filename)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding=\"utf-8\", sep=None, engine='python')\n",
        "            df.columns = df.columns.str.strip()\n",
        "\n",
        "            if \"Utterance\" not in df.columns or \"Sub topic\" not in df.columns:\n",
        "                raise KeyError(f\"Missing required columns in: {filename}\")\n",
        "\n",
        "            metadata_df = df.iloc[-3:].copy() if len(df) >= 3 else pd.DataFrame()\n",
        "            dialogue_df = df.iloc[:-3].copy() if len(df) >= 3 else df.copy()\n",
        "\n",
        "            # Filter out 'inactive' rows\n",
        "            dialogue_df = dialogue_df[~dialogue_df[\"Sub topic\"].str.lower().eq(\"inactive\")]\n",
        "\n",
        "            if dialogue_df.empty:\n",
        "                print(f\"⚠️ No valid dialogue in: {filename}\")\n",
        "                continue\n",
        "\n",
        "            if \"Type\" not in dialogue_df.columns:\n",
        "                raise KeyError(f\"Missing 'Type' column in: {filename}\")\n",
        "\n",
        "            # Process each row\n",
        "            def process_row(row):\n",
        "                utterance = str(row[\"Utterance\"]).strip()\n",
        "                role = \"Patient\" if row[\"Type\"] == \"P\" else \"Therapist\"\n",
        "                utterance = grammar_replace(utterance, role)\n",
        "                utterance = replace_you_and_your(utterance, role)\n",
        "                return utterance.lower()\n",
        "\n",
        "            dialogue_df[\"Utterance\"] = dialogue_df.apply(process_row, axis=1)\n",
        "\n",
        "            # Add sentiment\n",
        "            sentiments = sentiment_pipeline(dialogue_df[\"Utterance\"].tolist())\n",
        "            dialogue_df[\"Sentiment\"] = [s[\"label\"].lower() for s in sentiments]\n",
        "\n",
        "            final_df = pd.concat([dialogue_df, metadata_df], ignore_index=True)\n",
        "            output_file = os.path.join(output_path, filename)\n",
        "            final_df.to_csv(output_file, index=False)\n",
        "            processed_files.append(os.path.splitext(filename)[0])\n",
        "            print(f\"Saved: {output_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    # Save log file\n",
        "    with open(log_file_path, \"w\") as f:\n",
        "        for name in processed_files:\n",
        "            f.write(name + \"\\n\")\n",
        "\n",
        "    print(f\"\\nDone! {len(processed_files)} files processed.\")\n",
        "    print(f\"Log saved to: {log_file_path}\")\n"
      ],
      "metadata": {
        "id": "sfMe2PipNWAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_path = \"/content/drive/MyDrive/MEMO_KDD_2022/Train\"\n",
        "output_path = \"/content/drive/MyDrive/MEMO_KDD_2022/process_sentiment/training\"\n",
        "train = \"/content/drive/MyDrive/MEMO_KDD_2022/process_sentiment/train.txt\"\n",
        "\n",
        "preprocess_csv_files(input_path, output_path, train)\n"
      ],
      "metadata": {
        "id": "h210E-dnkhET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/drive/MyDrive/MEMO_KDD_2022/Validation\"\n",
        "output_path = \"/content/drive/MyDrive/MEMO_KDD_2022/process_sentiment/validating\"\n",
        "train = \"/content/drive/MyDrive/MEMO_KDD_2022/process_sentiment/val.txt\"\n",
        "\n",
        "preprocess_csv_files(input_path, output_path, train)\n"
      ],
      "metadata": {
        "id": "2HaWKgR7k_B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/drive/MyDrive/MEMO_KDD_2022/Test\"\n",
        "output_path = \"/content/drive/MyDrive/MEMO_KDD_2022/process_sentiment/testing\"\n",
        "train = \"/content/drive/MyDrive/MEMO_KDD_2022/process_sentiment/test.txt\"\n",
        "\n",
        "preprocess_csv_files(input_path, output_path, train)\n"
      ],
      "metadata": {
        "id": "LxufFX1DoBli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IJSOZ2QtFMba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FgYn0HxNtIq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BU5D4VNqtIcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nZSx-UumtISA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zr2jqdmStICx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATING DATASET"
      ],
      "metadata": {
        "id": "BqM_4FeRtJ2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re_EXzxUtLbS",
        "outputId": "31710856-19d1-4c94-b87e-5bd417c4f92b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def process_csv_folder(folder_path, output_filename='train.csv'):\n",
        "    final_data = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".csv\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            df = pd.read_csv(file_path)  # Normal comma-separated CSV\n",
        "\n",
        "            # Extract summary, primary_topic, secondary_topic\n",
        "            summary_row = df[df['Utterance'] == 'summary']\n",
        "            primary_row = df[df['Utterance'] == 'primary_topic']\n",
        "            secondary_row = df[df['Utterance'] == 'secondary_topic']\n",
        "\n",
        "            summary = summary_row.iloc[0, 1] if not summary_row.empty else np.nan\n",
        "            primary_topic = primary_row.iloc[0, 1] if not primary_row.empty else np.nan\n",
        "            secondary_topic = secondary_row.iloc[0, 1] if not secondary_row.empty else np.nan\n",
        "\n",
        "            # Drop these metadata rows\n",
        "            df = df[~df['Utterance'].isin(['summary', 'primary_topic', 'secondary_topic'])]\n",
        "\n",
        "            # Combine all utterances\n",
        "            combined_utterances = ' '.join(df['Utterance'].dropna().astype(str).tolist())\n",
        "\n",
        "            final_data.append({\n",
        "                'utterances': combined_utterances,\n",
        "                'summary': summary,\n",
        "                'primary_topic': primary_topic,\n",
        "                'secondary_topic': secondary_topic\n",
        "            })\n",
        "\n",
        "    # Create a combined DataFrame\n",
        "    combined_df = pd.DataFrame(final_data)\n",
        "\n",
        "    # Save to CSV\n",
        "    output_path = os.path.join(\"/content/drive/MyDrive/MEMO_KDD_2022/Processed_new\", output_filename)\n",
        "    combined_df.to_csv(output_path, index=False)\n",
        "\n",
        "    print(f\"Combined dataset saved to: {output_path}\")\n",
        "    return combined_df\n"
      ],
      "metadata": {
        "id": "0a1qauwKtYDo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/MEMO_KDD_2022/Processed_new/training\""
      ],
      "metadata": {
        "id": "1aQ0n0W-uy2J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your actual folder path\n",
        "combined_df = process_csv_folder(path)\n"
      ],
      "metadata": {
        "id": "vPfnTdtXvAAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/MEMO_KDD_2022/Processed_new/validating\"\n",
        "combined_df = process_csv_folder(path, \"valid.csv\")"
      ],
      "metadata": {
        "id": "mzr8v9DuvEdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/MEMO_KDD_2022/Processed_new/testing\"\n",
        "combined_df = process_csv_folder(path, \"test.csv\")"
      ],
      "metadata": {
        "id": "6Corhbsxvui6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5xNslZv5v4Js"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}