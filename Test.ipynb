{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "\n",
    "!pip install transformers sentencepiece torch numpy pandas nltk bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, DataCollatorForSeq2Seq, PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import os\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "from bert_score import score\n",
    "import nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add paths\n",
    "test_input = \"/path/to/input/folder/containing/csv\"\n",
    "output_file = \"path/for/processed/ouput/csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of the csv files to create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_counseling_component(sub_topic):\n",
    "  if sub_topic == \"symp/reasoning\":\n",
    "    return \"SH\"  # Symptom & History\n",
    "  elif sub_topic == \"routine\":\n",
    "    return \"RT\"  # Reflecting\n",
    "  elif sub_topic == \"inactive\":\n",
    "    return \"DF\"  # Discussion Filler\n",
    "  else:\n",
    "    return \"PD\"  # Default to Patient Discovery if unknown\n",
    "\n",
    "\n",
    "def create_dataset(input_path, output_path):\n",
    "  final_data = []\n",
    "\n",
    "  for filename in os.listdir(input_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "      # print(f\"Processing file: {filename}\")\n",
    "      file_path = os.path.join(input_path, filename)\n",
    "      df = pd.read_csv(file_path)\n",
    "\n",
    "      # extracting targeted set, primary and secondary topic\n",
    "      summary_text, primary_topic, secondary_topic = None, None, None\n",
    "      for i in range(1, 4): \n",
    "        if \"summary\" in str(df.iloc[-i, 0]).lower():\n",
    "          summary_text = df.iloc[-i, 1]\n",
    "        elif \"primary topic\" in str(df.iloc[-i, 0]).lower():\n",
    "          primary_topic = df.iloc[-i, 1]\n",
    "        elif \"secondary topic\" in str(df.iloc[-i, 0]).lower():\n",
    "          secondary_topic = df.iloc[-i, 1]\n",
    "      # print(summary_text)\n",
    "      # print(primary_topic)\n",
    "      # print(secondary_topic)\n",
    "\n",
    "      # Remove last three rows\n",
    "      df_cleaned = df.iloc[:-3]\n",
    "      df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "      df_cleaned = df_cleaned[[\"Utterance\", \"Sub topic\"]]\n",
    "      # print(df_cleaned)\n",
    "\n",
    "      df_cleaned[\"Counseling_Component\"] = df_cleaned[\"Sub topic\"].apply(assign_counseling_component)\n",
    "\n",
    "      # Combine utterances into a single conversation text\n",
    "      full_conversation = \" \".join(df_cleaned[\"Utterance\"].astype(str).fillna(\"\").tolist())\n",
    "\n",
    "      t5_input = f\"Summarize: {full_conversation}\"\n",
    "      t5_target = summary_text \n",
    "      # print(t5_input)\n",
    "      # print(t5_target)\n",
    "\n",
    "      final_data.append({\"Index\": len(final_data), \"Input\": t5_input, \"Output\": t5_target})\n",
    "      \n",
    "  df_final = pd.DataFrame(final_data)\n",
    "  df_final.to_csv(output_path, index=False)\n",
    "  print(f\"Processed and saved all data to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = test_input\n",
    "output_file = output_file\n",
    "create_dataset(input_folder, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating test dataframe\n",
    "test_df = pd.read_csv(output_file)\n",
    "test_df.fillna(\"\", inplace=True)\n",
    "test_df[\"Input\"] = test_df[\"Input\"].astype(str)\n",
    "test_df[\"Output\"] = test_df[\"Output\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Generating summaries using t5-small model\n",
    "def generate_summary(text):\n",
    "    input_text = \"summarize: \" + text \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(**inputs, max_length=100, num_beams=5, length_penalty=2.0)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "df[\"Generated_Summary\"] = df[\"Input\"].apply(generate_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the BLEU and BERT scores\n",
    "\n",
    "def compute_sentence_bleu(reference, hypothesis):\n",
    "    reference_tokens = [reference.split()]\n",
    "    hypothesis_tokens = hypothesis.split()\n",
    "    smoothing = SmoothingFunction().method4 \n",
    "    weights = (0.25, 0.25, 0.25, 0.25) \n",
    "    return sentence_bleu(reference_tokens, hypothesis_tokens, weights=weights, smoothing_function=smoothing)\n",
    "\n",
    "# BLEU scores\n",
    "df[\"BLEU_Score\"] = df.apply(lambda row: compute_sentence_bleu(row[\"Output\"], row[\"Generated_Summary\"]), axis=1)\n",
    "\n",
    "references = [[ref.split()] for ref in df[\"Output\"].tolist()]\n",
    "hypotheses = [hyp.split() for hyp in df[\"Generated_Summary\"].tolist()]\n",
    "\n",
    "final_bleu = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method4)\n",
    "\n",
    "# Compute BERTScore\n",
    "P, R, F1 = score(df[\"Generated_Summary\"].tolist(), df[\"Output\"].tolist(), lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "df[\"BERTScore_F1\"] = F1.tolist()\n",
    "\n",
    "final_bert = F1.mean().item()\n",
    "\n",
    "print(f\"Final Corpus BLEU Score: {final_bleu:.4f}\")\n",
    "print(f\"Final BERTScore F1: {final_bert:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "model_name = \"t5-large\" \n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Generating summaries for each data point\n",
    "def generate_summary(text):\n",
    "    input_text = \"Summarize: \" + text\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).input_ids\n",
    "    outputs = model.generate(input_ids, max_length=150, num_beams=5, length_penalty=1.0, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "df[\"Generated_Summary\"] = df[\"Input\"].apply(generate_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the BLEU and BERT scores\n",
    "def compute_sentence_bleu(reference, hypothesis):\n",
    "    reference_tokens = [reference.split()]\n",
    "    hypothesis_tokens = hypothesis.split()\n",
    "    smoothing = SmoothingFunction().method4 \n",
    "    weights = (0.25, 0.25, 0.25, 0.25) \n",
    "    return sentence_bleu(reference_tokens, hypothesis_tokens, weights=weights, smoothing_function=smoothing)\n",
    "\n",
    "# BLEU scores\n",
    "df[\"BLEU_Score\"] = df.apply(lambda row: compute_sentence_bleu(row[\"Output\"], row[\"Generated_Summary\"]), axis=1)\n",
    "\n",
    "references = [[ref.split()] for ref in df[\"Output\"].tolist()]\n",
    "hypotheses = [hyp.split() for hyp in df[\"Generated_Summary\"].tolist()]\n",
    "\n",
    "final_bleu = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method4)\n",
    "# Compute BERTScore\n",
    "P, R, F1 = score(df[\"Generated_Summary\"].tolist(), df[\"Output\"].tolist(), lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "df[\"BERTScore_F1\"] = F1.tolist()\n",
    "\n",
    "final_bert = F1.mean().item()\n",
    "\n",
    "print(f\"Final Corpus BLEU Score: {final_bleu:.4f}\")\n",
    "print(f\"Final BERTScore F1: {final_bert:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling of BERT and BLEU scores : https://github.com/Tiiiger/bert_score/blob/master/journal/rescale_baseline.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the BLEU and BERT scores and scaling them \n",
    "\n",
    "def compute_sentence_bleu(reference, hypothesis):\n",
    "    reference_tokens = [nltk.word_tokenize(reference)]  \n",
    "    hypothesis_tokens = nltk.word_tokenize(hypothesis)  \n",
    "    smoothing = SmoothingFunction().method4\n",
    "    return sentence_bleu(reference_tokens, hypothesis_tokens, smoothing_function=smoothing)\n",
    "\n",
    "# BLEU scores\n",
    "df[\"BLEU_Score\"] = df.apply(lambda row: compute_sentence_bleu(row[\"Output\"], row[\"Generated_Summary\"]), axis=1)\n",
    "\n",
    "df[\"BLEU_Score\"] = (df[\"BLEU_Score\"] - df[\"BLEU_Score\"].min()) / (df[\"BLEU_Score\"].max() - df[\"BLEU_Score\"].min())\n",
    "\n",
    "references = [[nltk.word_tokenize(ref)] for ref in df[\"Output\"].tolist()]\n",
    "hypotheses = [nltk.word_tokenize(hyp) for hyp in df[\"Generated_Summary\"].tolist()]\n",
    "final_bleu = corpus_bleu(references, hypotheses, smoothing_function=SmoothingFunction().method4)\n",
    "\n",
    "final_bleu = (final_bleu - 0) / (1 - 0) \n",
    "\n",
    "# BERT Score\n",
    "P, R, F1 = score(df[\"Generated_Summary\"].astype(str).tolist(), df[\"Output\"].astype(str).tolist(), lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "df[\"BERTScore_F1\"] = (F1 - np.min(F1.numpy())) / (np.max(F1.numpy()) - np.min(F1.numpy()))\n",
    "final_bert = df[\"BERTScore_F1\"].mean()\n",
    "\n",
    "print(f\"Final Corpus BLEU Score (Scaled): {final_bleu:.4f}\")\n",
    "print(f\"Final BERTScore F1 (Scaled): {final_bert:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pegasus-large model\n",
    "model_name = \"google/pegasus-large\" \n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "if device == \"cuda\":\n",
    "  model = model.to(device)\n",
    "\n",
    "def generate_summary(text):\n",
    "    inputs = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\", max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(**inputs, max_length=100, num_beams=5, length_penalty=2.0)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "df[\"Generated_Summary\"] = df[\"Input\"].apply(generate_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing BLEU and BERT scores\n",
    "def compute_sentence_bleu(reference, hypothesis):\n",
    "    reference_tokens = [nltk.word_tokenize(reference)] \n",
    "    hypothesis_tokens = nltk.word_tokenize(hypothesis) \n",
    "    smoothing = SmoothingFunction().method4\n",
    "    return sentence_bleu(reference_tokens, hypothesis_tokens, smoothing_function=smoothing)\n",
    "\n",
    "# Compute BLEU scores\n",
    "df[\"BLEU_Score\"] = df.apply(lambda row: compute_sentence_bleu(row[\"Output\"], row[\"Generated_Summary\"]), axis=1)\n",
    "\n",
    "df[\"BLEU_Score\"] = (df[\"BLEU_Score\"] - df[\"BLEU_Score\"].min()) / (df[\"BLEU_Score\"].max() - df[\"BLEU_Score\"].min())\n",
    "\n",
    "# Compute Corpus BLEU\n",
    "references = [[nltk.word_tokenize(ref)] for ref in df[\"Output\"].tolist()]\n",
    "hypotheses = [nltk.word_tokenize(hyp) for hyp in df[\"Generated_Summary\"].tolist()]\n",
    "final_bleu = corpus_bleu(references, hypotheses, smoothing_function=SmoothingFunction().method4)\n",
    "\n",
    "final_bleu = (final_bleu - 0) / (1 - 0)\n",
    "\n",
    "# Compute BERTScore\n",
    "P, R, F1 = score(df[\"Generated_Summary\"].astype(str).tolist(), df[\"Output\"].astype(str).tolist(), lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "df[\"BERTScore_F1\"] = (F1 - np.min(F1.numpy())) / (np.max(F1.numpy()) - np.min(F1.numpy()))\n",
    "final_bert = df[\"BERTScore_F1\"].mean()\n",
    "\n",
    "print(f\"Final Corpus BLEU Score (Scaled): {final_bleu:.4f}\")\n",
    "print(f\"Final BERTScore F1 (Scaled): {final_bert:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
